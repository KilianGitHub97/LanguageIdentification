{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17761,
     "status": "ok",
     "timestamp": 1665763866856,
     "user": {
      "displayName": "Kiram",
      "userId": "11375370508711758667"
     },
     "user_tz": -120
    },
    "id": "aX_oN9JViMr-",
    "outputId": "0b70bb57-292e-4732-f95b-b2b3f9c7df92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textaugment in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: googletrans in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from textaugment) (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from textaugment) (1.23.4)\n",
      "Requirement already satisfied: gensim in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from textaugment) (4.2.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from textaugment) (3.7)\n",
      "Requirement already satisfied: textblob in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from textaugment) (0.17.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from gensim->textaugment) (6.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from gensim->textaugment) (1.9.2)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from gensim->textaugment) (0.29.28)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from googletrans->textaugment) (0.13.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (1.3.0)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (2022.10.1)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (0.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (2022.9.24)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (2.10)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (3.0.4)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (1.5.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (3.2.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (0.9.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (3.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from nltk->textaugment) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from nltk->textaugment) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from nltk->textaugment) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from nltk->textaugment) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from click->nltk->textaugment) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kiram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: eli5 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: six in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from eli5) (1.16.0)\n",
      "Requirement already satisfied: attrs>17.1.0 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from eli5) (22.1.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from eli5) (0.20.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from eli5) (1.9.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from eli5) (1.1.2)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from eli5) (3.1.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from eli5) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from eli5) (1.23.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from jinja2>=3.0.0->eli5) (2.1.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn>=0.20->eli5) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kiram\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from scikit-learn>=0.20->eli5) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import csv\n",
    "import re\n",
    "import unicodedata as uc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from io import StringIO\n",
    "import requests\n",
    "\n",
    "import math\n",
    "!pip install textaugment\n",
    "import textaugment #https://pypi.org/project/textaugment/\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "!pip install eli5\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YqY4qyneivft"
   },
   "outputs": [],
   "source": [
    "url_train_dev = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTOZ2rC82rhNsJduoyKYTsVeH6ukd7Bpxvxn_afOibn3R-eadZGXu82eCU9IRpl4CK_gefEGsYrA_oM/pub?gid=1863430984&single=true&output=tsv'\n",
    "url_test = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vT-KNR9nuYatLkSbzSRgpz6Ku1n4TN4w6kKmFLkA6QJHTfQzmX0puBsLF7PAAQJQAxUpgruDd_RRgK7/pub?gid=417546901&single=true&output=tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UcHnXqC_hYA9"
   },
   "outputs": [],
   "source": [
    "class DataDownloader:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def load_dataset(self, url) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: Downloads data from URL and creates pandas dataframe\n",
    "    SOURCE: https://colab.research.google.com/drive/18eTnEKKZFP91nNfmnBFpQ2baw8GmOxtI?usp=sharing&pli=1#scrollTo=BCLdrs43pJC1\n",
    "    \"\"\"\n",
    "\n",
    "    r = requests.get(url)\n",
    "    data = r.content.decode('utf8')\n",
    "    df = pd.read_csv(StringIO(data), sep='\\t')\n",
    "    df.columns = ['tweet', 'label']\n",
    "    return df\n",
    "\n",
    "DD = DataDownloader()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mTThDpgti3bS"
   },
   "outputs": [],
   "source": [
    "class PreProcessing:\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def clean_tweets(self, data) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: Preprocesses tweets such that only language specific text is in it.\n",
    "    Uses __delete_links, __unicode_normalization, __delete_numbers, __replace_meaningless_char\n",
    "    Note that list comprehentions are used instead of pandas.apply() as apply is not vectorized and\n",
    "    list comprehentions make the code approx. 4x faster.\n",
    "    \"\"\"\n",
    "    temp = data.copy(deep=True)\n",
    "    temp[\"tweet\"] = [self.__delete_links(tweet) for tweet in temp[\"tweet\"].to_list()]\n",
    "    temp[\"tweet\"] = [self.__delete_twitter_usernames(tweet) for tweet in temp[\"tweet\"].to_list()]\n",
    "    temp[\"tweet\"] = [self.__delete_hashtags(tweet) for tweet in temp[\"tweet\"].to_list()]\n",
    "    temp[\"tweet\"] = [self.__unicode_normalization(tweet) for tweet in temp[\"tweet\"].to_list()]\n",
    "    temp[\"tweet\"] = [self.__delete_numbers(tweet) for tweet in temp[\"tweet\"].to_list()]\n",
    "    temp[\"tweet\"] = [self.__replace_meaningless_char(tweet) for tweet in temp[\"tweet\"].to_list()]\n",
    "    temp[\"tweet\"] = [self.__lowercase_string(tweet) for tweet in temp[\"tweet\"].to_list()]\n",
    "\n",
    "    #some tweets only consist of a single link. By cleaning out all links, these files now only contain spaces.\n",
    "    #Here these corrupt rows are being deleted\n",
    "    temp = temp[temp[\"tweet\"] != \"  \"]\n",
    "    temp = temp[temp[\"tweet\"] != \" \"]\n",
    "    temp = temp[temp[\"tweet\"] != \"\"]\n",
    "\n",
    "    return temp\n",
    "\n",
    "  def __delete_links(self, text) -> str:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: Deletes all links from a message\n",
    "    Works with any language (e.g. Chinese and Arab)\n",
    "    SOURCE: https://devenum.com/how-to-extract-url-from-a-string-in-python/\n",
    "    \"\"\"\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    return re.sub(pattern = regex, repl = \"\", string = text)\n",
    "  \n",
    "  def __delete_twitter_usernames(self, text) -> str:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: Deletes all twitter Usernames starting with @\n",
    "    This is needed as twitter usernames are meaningless, don't get cut out in the other methods\n",
    "    and are not language specific!\n",
    "    \"\"\"\n",
    "    regex = r\"@\\w*\"\n",
    "    return re.sub(pattern = regex, repl = \"\", string = text)\n",
    "\n",
    "  def __delete_hashtags(self, text) -> str:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: Deletes all twitter hashtags\n",
    "    This is needed as hashtags in any language are often written in english which potentially \n",
    "    leads to worse predictive accuracy.\n",
    "    \"\"\"\n",
    "    regex = r\"#\\w*\"\n",
    "    return re.sub(pattern = regex, repl = \"\", string = text)\n",
    "\n",
    "  def __unicode_normalization(self, text) -> str:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: Normalizes all unicode characters\n",
    "    Works with any language (e.g. Chinese and Arab)\n",
    "    FURTHER STUDY: https://www.unicode.org/reports/tr15/, https://www.evernote.com/shard/s223/client/snv?noteGuid=f1c62270-a999-415f-ad24-39996d829ad5&noteKey=d379441c7671371c744cd5266c932406&sn=https%3A%2F%2Fwww.evernote.com%2Fshard%2Fs223%2Fsh%2Ff1c62270-a999-415f-ad24-39996d829ad5%2Fd379441c7671371c744cd5266c932406&title=2.%2BNLP%2BPipeline%2B%257C%2BPractical%2BNatural%2BLanguage%2BProcessing\n",
    "    \"\"\"\n",
    "    return uc.normalize(\"NFKD\", text) #canonical\n",
    "\n",
    "  def __delete_numbers(self, text) -> str:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: takes a string and returns it without any numbers in it.\n",
    "    Works with any language (e.g. Chinese and Arab)\n",
    "    \"\"\"\n",
    "    regex = r\"[0-9]*\"\n",
    "    return re.sub(pattern = regex, repl = \"\", string = text)\n",
    "\n",
    "  def __replace_meaningless_char(self, text) -> str:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: takes a string and returns it without any special characters such as @, #, [, ,( in it.\n",
    "    Works with any language (e.g. Chinese and Arab)\n",
    "    SOURCE: https://stackoverflow.com/questions/1576789/in-regex-what-does-w-mean\n",
    "    \"\"\"\n",
    "    regex = r\"[\\W_]+\"\n",
    "    return re.sub(regex, \" \", text)\n",
    "\n",
    "  def __lowercase_string(self, text):\n",
    "    \"\"\"\n",
    "    DESCRIPTION: Resulting string has only lower letters.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "PP = PreProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pJDCdMEviDLm"
   },
   "outputs": [],
   "source": [
    "class DataAugmentation:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.eda = textaugment.EDA()\n",
    "\n",
    "  def augment_labels(self, data, min_labels) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: upsamples labels from the dataset, which are below \"min_labels\". This is done to\n",
    "    improve model robustness.\n",
    "    Note that this method should only be run on the training data and not on the test data.\n",
    "\n",
    "    REQUIREMENT: data must be a dataset with two rows: tweet & label\n",
    "    \"\"\"\n",
    "\n",
    "    temp = data.copy(deep=True)\n",
    "\n",
    "    #identify labels that must be upsampled\n",
    "    #here we want each category to have at least \"min_labels\" samples\n",
    "    labels = data[\"label\"].value_counts()\n",
    "    labels_under_min_labels = labels[labels < min_labels]\n",
    "\n",
    "    for label, num in zip(labels_under_min_labels.index, labels_under_min_labels):\n",
    "      \n",
    "      #compute number of upsamples needed to reach \"min_labels\" values per category\n",
    "      #e.g. the data holds 349 english tweets, we need 1000-349 = 651 upsamples \n",
    "      to_be_upsampled = min_labels-num\n",
    "      to_be_upsampled = math.ceil(to_be_upsampled/2)\n",
    "\n",
    "      #for each label that needs upsampling, select \"to_be_upsampled\" (e.g. 651) amounts of tweets\n",
    "      #half of them will be upsampled with random_deletion (rd), the other half with random_swap (rs)\n",
    "      single_language = temp[temp[\"label\"] == label]\n",
    "      rd = single_language.sample(n = to_be_upsampled, replace = True)\n",
    "      rs = single_language.sample(n = to_be_upsampled, replace = True)\n",
    "\n",
    "      #upsample\n",
    "      rd_new = [self.__random_deletion(tweet) for tweet in rd[\"tweet\"].to_list()]\n",
    "      rs_new = [self.__random_swap(tweet) for tweet in rs[\"tweet\"].to_list()]\n",
    "\n",
    "      #make new dataset with upsampled data\n",
    "      rd_new.extend(rs_new)\n",
    "\n",
    "      #it seems that some single words get saved as a list. Therefore, here, all the tweets are \n",
    "      #being converted to a string\n",
    "      #e.g. [wifhekafhe] --> wofhekafhe\n",
    "      rd_new_str = list()\n",
    "      for sentence in rd_new:\n",
    "        rd_new_str.append(str(sentence))\n",
    "\n",
    "      #make a dataframe with the augmented data \n",
    "      to_be_appended_to_temp = pd.DataFrame({\n",
    "          \"tweet\": rd_new_str,\n",
    "          \"label\": [label] * len(rd_new)\n",
    "      })\n",
    "\n",
    "      #append upsampled data to the original data\n",
    "      temp = pd.concat([temp, to_be_appended_to_temp], axis = 0)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "  def __random_deletion(self, text) -> str:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: Uses Random Deletion to upsample a tweet\n",
    "    Note that all techniques using WordNet or other language specific augmentation cannot \n",
    "    (without substancial effort) be used with multilingual data\n",
    "    SOURCE: https://towardsdatascience.com/these-are-the-easiest-data-augmentation-techniques-in-natural-language-processing-you-can-think-of-88e393fd610\n",
    "    \"\"\"\n",
    "    return self.eda.random_deletion(text, p=0.2)\n",
    "\n",
    "  def __random_swap(self, text) -> str:\n",
    "    \"\"\"\n",
    "    DESCRIPTION: Uses Random Swap to upsample a tweet\n",
    "    Note that all techniques using WordNet or other language specific augmentation cannot \n",
    "    (without substancial effort) be used with multilingual data\n",
    "    SOURCE: https://towardsdatascience.com/these-are-the-easiest-data-augmentation-techniques-in-natural-language-processing-you-can-think-of-88e393fd610\n",
    "    \"\"\"\n",
    "    return self.eda.random_swap(text)\n",
    "\n",
    "DA = DataAugmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7DPnjZFsVbUo"
   },
   "outputs": [],
   "source": [
    "#custom sklearn class that counts the number of features of each tweet.\n",
    "class NumCharTransformer(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    DESCRIPTION: This class is a custom sklearn class used to engineer the \n",
    "    length-of-a-tweet feature. It inherits from BaseEstimator and TransformerMixin\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X, y = None) -> np.array:\n",
    "    X_ = X.copy()\n",
    "\n",
    "    #count the number of characters of each tweet\n",
    "    out = np.array([len(i) for i in np.squeeze(X_)])\n",
    "\n",
    "    #reshape into 2d array. This is necessary to fit the dimensions of the CountVectorizer\n",
    "    out = out.reshape((-1, 1))\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AHEfN7kCjx0i"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "df_train = DD.load_dataset(url_train_dev)\n",
    "df_test = DD.load_dataset(url_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1665764751089,
     "user": {
      "displayName": "Kiram",
      "userId": "11375370508711758667"
     },
     "user_tz": -120
    },
    "id": "r8nrHUbAlx91",
    "outputId": "78f4bfb4-e91a-408a-a3e6-43145534e440"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42261</th>\n",
       "      <td>@ziha_awsme @MinaSyamina asal liam? Alhamdulil...</td>\n",
       "      <td>ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22443</th>\n",
       "      <td>“@Sororita : #Resistencia #Cojedes #8J Hospita...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17947</th>\n",
       "      <td>THANK U LORD ILY ALL OMFG THE PAIN IM SO URRRR...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37953</th>\n",
       "      <td>すっごく苦しい</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10830</th>\n",
       "      <td>Damn 😂😂😂</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13599</th>\n",
       "      <td>These niggas just putting out these thots nude...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15770</th>\n",
       "      <td>Why so attractive😍😏❤️ http://t.co/GAuLKFJwVc</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10068</th>\n",
       "      <td>@DCKitty @SweetPeaBeardie @bijntje @spike_cat ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37708</th>\n",
       "      <td>明日も早いのに寝れないなぁ</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8772</th>\n",
       "      <td>Photo: New Tattoo done Today by EMDM.. Thank u...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet label\n",
       "42261  @ziha_awsme @MinaSyamina asal liam? Alhamdulil...    ms\n",
       "22443  “@Sororita : #Resistencia #Cojedes #8J Hospita...    es\n",
       "17947  THANK U LORD ILY ALL OMFG THE PAIN IM SO URRRR...    en\n",
       "37953                                            すっごく苦しい    ja\n",
       "10830                                           Damn 😂😂😂    en\n",
       "13599  These niggas just putting out these thots nude...    en\n",
       "15770       Why so attractive😍😏❤️ http://t.co/GAuLKFJwVc    en\n",
       "10068  @DCKitty @SweetPeaBeardie @bijntje @spike_cat ...    en\n",
       "37708                                      明日も早いのに寝れないなぁ    ja\n",
       "8772   Photo: New Tattoo done Today by EMDM.. Thank u...    en"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first glance at the data\n",
    "#seems that there are many links that have to be deleted as they are not language specific\n",
    "#there also seem to be smileys and #/@ that are not language specific\n",
    "df_train.sample(n=10, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1665764755170,
     "user": {
      "displayName": "Kiram",
      "userId": "11375370508711758667"
     },
     "user_tz": -120
    },
    "id": "KiYq4E7Jo1hJ",
    "outputId": "f7edb38a-7265-4d60-c79a-f134a688c14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet', 'label'], dtype='object')\n",
      "(52675, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1665764755782,
     "user": {
      "displayName": "Kiram",
      "userId": "11375370508711758667"
     },
     "user_tz": -120
    },
    "id": "6VdGCtWknI_J",
    "outputId": "0a2692f6-a3ea-432a-a2af-d05939206d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en         18508\n",
      "ja         10421\n",
      "es          5930\n",
      "und         4537\n",
      "id          3006\n",
      "           ...  \n",
      "tn             1\n",
      "dv             1\n",
      "ta_LATN        1\n",
      "si             1\n",
      "ko_LATN        1\n",
      "Name: label, Length: 69, dtype: int64\n",
      "AxesSubplot(0.125,0.11;0.775x0.77)\n"
     ]
    }
   ],
   "source": [
    "#labels distibution is heavily skewed\n",
    "#some levels only occur once, we must likely upsample these categories.\n",
    "label_dist = df_train[\"label\"].value_counts()\n",
    "print(label_dist)\n",
    "print(label_dist.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1665764758521,
     "user": {
      "displayName": "Kiram",
      "userId": "11375370508711758667"
     },
     "user_tz": -120
    },
    "id": "pS2a96ENsRLk",
    "outputId": "e73a89d8-ac40-48c8-9ffb-703d4771eb42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of unique labels of the data set\n",
    "len(label_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1665764759370,
     "user": {
      "displayName": "Kiram",
      "userId": "11375370508711758667"
     },
     "user_tz": -120
    },
    "id": "1BXSk8iEomlz",
    "outputId": "717ec52d-2701-45b6-9ed7-6988c0a5d9c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2)\n",
      "(0, 2)\n"
     ]
    }
   ],
   "source": [
    "#There are no empty tweets or missing labels\n",
    "print(df_train[df_train[\"tweet\"].isna()].shape)\n",
    "print(df_train[df_train[\"label\"].isna()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 1529,
     "status": "ok",
     "timestamp": 1665764762403,
     "user": {
      "displayName": "Kiram",
      "userId": "11375370508711758667"
     },
     "user_tz": -120
    },
    "id": "XhZUPIf_wUGu",
    "outputId": "a63eaea0-2307-4393-a2f3-cda007fee2f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20519</th>\n",
       "      <td>ha rita ora iggy azalea ellie goulding chris b...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27833</th>\n",
       "      <td>fait comme si j e tait pas la</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9437</th>\n",
       "      <td>scolari should not try to use neymar at all we...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43656</th>\n",
       "      <td>olha em meus olhos e veja o brilho de loucura ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31346</th>\n",
       "      <td>恐らく私の中て の忍足侑士のイメーシ は直線なんた ろうな 足を揃えた立ち姿か 綺麗というか...</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24115</th>\n",
       "      <td>he son ado con le pedi a saltando que se echar...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38581</th>\n",
       "      <td>テスト勉強は帰ってからやるとしようかね</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16469</th>\n",
       "      <td>i m getting taco bell</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50452</th>\n",
       "      <td>encuentros latinas bitches crea tu perfil</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15590</th>\n",
       "      <td>does anyone hates it when your shit is left ha...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet label\n",
       "20519  ha rita ora iggy azalea ellie goulding chris b...    en\n",
       "27833                     fait comme si j e tait pas la     fr\n",
       "9437   scolari should not try to use neymar at all we...    en\n",
       "43656  olha em meus olhos e veja o brilho de loucura ...    pt\n",
       "31346  恐らく私の中て の忍足侑士のイメーシ は直線なんた ろうな 足を揃えた立ち姿か 綺麗というか...    ja\n",
       "24115  he son ado con le pedi a saltando que se echar...    es\n",
       "38581                                テスト勉強は帰ってからやるとしようかね    ja\n",
       "16469                             i m getting taco bell     en\n",
       "50452         encuentros latinas bitches crea tu perfil    und\n",
       "15590  does anyone hates it when your shit is left ha...    en"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the tweets according to the process of the PreProcessing object\n",
    "df_train = PP.clean_tweets(df_train)\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1531,
     "status": "ok",
     "timestamp": 1665764763927,
     "user": {
      "displayName": "Kiram",
      "userId": "11375370508711758667"
     },
     "user_tz": -120
    },
    "id": "F5sJmg3oddfm",
    "outputId": "a61770b3-1076-4cc1-c50a-1cd2be7ec2db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107627, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#augment labels according to the procedure from the DataAugmentation object\n",
    "df_train = DA.augment_labels(df_train, min_labels = 1000)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1665764766506,
     "user": {
      "displayName": "Kiram",
      "userId": "11375370508711758667"
     },
     "user_tz": -120
    },
    "id": "2igsbaq2s0UQ",
    "outputId": "8224e3c5-b2cd-4b94-b946-f15b9203e078"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>rt हम त म द न ल ट मनम हन म कह दम थ छ त र प ल ज...</td>\n",
       "      <td>hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>already</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>freuen uns u jeden die der es geschafft wieder...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24658</th>\n",
       "      <td>yo dando spoilers soy el mismo satan en la tie...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10270</th>\n",
       "      <td>what is the most beautiful language idk</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>بسم الله الذي لا يضر مع اسمه شيء في الا رض ولا...</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>不要眯啦 來喝一杯啦</td>\n",
       "      <td>zh-TW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29853</th>\n",
       "      <td>ngantuk</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>պատրաստվում լեվոնյանը մենահամերգի է ալլա</td>\n",
       "      <td>hy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43292</th>\n",
       "      <td>seu perfil foi visto por pessoas nas u ltimas ...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  label\n",
       "676    rt हम त म द न ल ट मनम हन म कह दम थ छ त र प ल ज...     hi\n",
       "3446                                            already      en\n",
       "277    freuen uns u jeden die der es geschafft wieder...     de\n",
       "24658  yo dando spoilers soy el mismo satan en la tie...     es\n",
       "10270           what is the most beautiful language idk      en\n",
       "1053   بسم الله الذي لا يضر مع اسمه شيء في الا رض ولا...     ar\n",
       "82                                            不要眯啦 來喝一杯啦  zh-TW\n",
       "29853                                           ngantuk      id\n",
       "963             պատրաստվում լեվոնյանը մենահամերգի է ալլա     hy\n",
       "43292  seu perfil foi visto por pessoas nas u ltimas ...     pt"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NHvKUsooreW0"
   },
   "outputs": [],
   "source": [
    "#split training data into train and validation using a 90/10 split and with shuffeling\n",
    "#note we are using preprocessed and augmented data.\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(df_train[\"tweet\"], df_train[\"label\"], test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1665764770914,
     "user": {
      "displayName": "Kiram",
      "userId": "11375370508711758667"
     },
     "user_tz": -120
    },
    "id": "H4IZ0ftF1I4t",
    "outputId": "45e8ccc6-eeb0-422d-c31a-72081e5669d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training set: 96864\n",
      "Length of the validation set: 10763\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the training set: {len(X_train)}\")\n",
    "print(f\"Length of the validation set: {len(X_validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AjlAaYFlrHMR"
   },
   "outputs": [],
   "source": [
    "#make a pipeline that engineers the length-of-a-tweet feature\n",
    "tweet_length = Pipeline([\n",
    "    (\"numtans\", NumCharTransformer()),\n",
    "    (\"scale\", StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mY1gVzsa5tfu"
   },
   "outputs": [],
   "source": [
    "#create a pipeline that engineers the Bag-of-words\n",
    "#note that clean_tweets() was not put into the pipeline (could be done with functionTransformer())\n",
    "#this is because it has no parameters to tweak and it would possibly create overhead with GridSearchCV\n",
    "bag_of_words = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        lowercase = False, \n",
    "        encoding='utf-8',\n",
    "        preprocessor=None, \n",
    "        tokenizer=None, \n",
    "        stop_words=None\n",
    "        )),\n",
    "    ('feature_selector', SelectKBest(\n",
    "        chi2, \n",
    "        k=50000\n",
    "        )),\n",
    "    ('tfidf', TfidfTransformer(\n",
    "        smooth_idf=True\n",
    "        ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "c_ZV_pKRsjT-"
   },
   "outputs": [],
   "source": [
    "#FeatureUnion is used to append the bag-of-words matrix to the tweet-length vector\n",
    "#both must have appropriate dimensionality\n",
    "all_features = FeatureUnion([\n",
    "    (\"len\", tweet_length),\n",
    "    (\"bagger\", bag_of_words)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "uGrZflKJtolJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;all&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;len&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;numtans&#x27;,\n",
       "                                                                  NumCharTransformer()),\n",
       "                                                                 (&#x27;scale&#x27;,\n",
       "                                                                  StandardScaler())])),\n",
       "                                                (&#x27;bagger&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                  CountVectorizer(lowercase=False)),\n",
       "                                                                 (&#x27;feature_selector&#x27;,\n",
       "                                                                  SelectKBest(k=50000,\n",
       "                                                                              score_func=&lt;function chi2 at 0x000001AD1C04E8C0&gt;)),\n",
       "                                                                 (&#x27;tfidf&#x27;,\n",
       "                                                                  TfidfTransformer())]))])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(8, 6, 1),\n",
       "                               max_iter=100, random_state=123))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-49\" type=\"checkbox\" ><label for=\"sk-estimator-id-49\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;all&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;len&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;numtans&#x27;,\n",
       "                                                                  NumCharTransformer()),\n",
       "                                                                 (&#x27;scale&#x27;,\n",
       "                                                                  StandardScaler())])),\n",
       "                                                (&#x27;bagger&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                  CountVectorizer(lowercase=False)),\n",
       "                                                                 (&#x27;feature_selector&#x27;,\n",
       "                                                                  SelectKBest(k=50000,\n",
       "                                                                              score_func=&lt;function chi2 at 0x000001AD1C04E8C0&gt;)),\n",
       "                                                                 (&#x27;tfidf&#x27;,\n",
       "                                                                  TfidfTransformer())]))])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(8, 6, 1),\n",
       "                               max_iter=100, random_state=123))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-50\" type=\"checkbox\" ><label for=\"sk-estimator-id-50\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">all: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;len&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;numtans&#x27;,\n",
       "                                                 NumCharTransformer()),\n",
       "                                                (&#x27;scale&#x27;, StandardScaler())])),\n",
       "                               (&#x27;bagger&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                 CountVectorizer(lowercase=False)),\n",
       "                                                (&#x27;feature_selector&#x27;,\n",
       "                                                 SelectKBest(k=50000,\n",
       "                                                             score_func=&lt;function chi2 at 0x000001AD1C04E8C0&gt;)),\n",
       "                                                (&#x27;tfidf&#x27;,\n",
       "                                                 TfidfTransformer())]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>len</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\" ><label for=\"sk-estimator-id-51\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumCharTransformer</label><div class=\"sk-toggleable__content\"><pre>NumCharTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\" ><label for=\"sk-estimator-id-52\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>bagger</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" ><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(lowercase=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-54\" type=\"checkbox\" ><label for=\"sk-estimator-id-54\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectKBest</label><div class=\"sk-toggleable__content\"><pre>SelectKBest(k=50000, score_func=&lt;function chi2 at 0x000001AD1C04E8C0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" ><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\" ><label for=\"sk-estimator-id-56\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(8, 6, 1), max_iter=100,\n",
       "              random_state=123)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('all',\n",
       "                 FeatureUnion(transformer_list=[('len',\n",
       "                                                 Pipeline(steps=[('numtans',\n",
       "                                                                  NumCharTransformer()),\n",
       "                                                                 ('scale',\n",
       "                                                                  StandardScaler())])),\n",
       "                                                ('bagger',\n",
       "                                                 Pipeline(steps=[('vect',\n",
       "                                                                  CountVectorizer(lowercase=False)),\n",
       "                                                                 ('feature_selector',\n",
       "                                                                  SelectKBest(k=50000,\n",
       "                                                                              score_func=<function chi2 at 0x000001AD1C04E8C0>)),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfTransformer())]))])),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(activation='tanh', hidden_layer_sizes=(8, 6, 1),\n",
       "                               max_iter=100, random_state=123))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the MLP model on the engineered features\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(8,6,1), max_iter=100,activation = 'tanh',solver='adam',random_state=123)\n",
    "pipeline = Pipeline([\n",
    "   (\"all\", all_features),\n",
    "   ('mlpclassifier', mlp) \n",
    "])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "eVwpYnbcrJul"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy from crossvalidation 0.3103629831516353\n",
      "standard deviation from crossvalidation 0.004298622769571196\n"
     ]
    }
   ],
   "source": [
    "#repeated K-fold Crossvalidation to get robust estimates\n",
    "#note this has not been run as it would take too much time\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv = RepeatedKFold(n_splits=2))\n",
    "print(f\"mean accuracy from crossvalidation {scores.mean()}\")\n",
    "print(f\"standard deviation from crossvalidation {scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2151,
     "status": "ok",
     "timestamp": 1665584921581,
     "user": {
      "displayName": "k sennr",
      "userId": "01480698591619251915"
     },
     "user_tz": -120
    },
    "id": "ujneavgOlpZt",
    "outputId": "bc12e6d0-e3f6-4f1c-ebd0-433839e48ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Training Data: 0.33149570531879746\n",
      "Performance on Validation Data: 0.32797547152280965\n"
     ]
    }
   ],
   "source": [
    "print(f\"Performance on Training Data: {pipeline.score(X_train, y_train)}\")\n",
    "print(f\"Performance on Validation Data: {pipeline.score(X_validation, y_validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "4a73otooGYRC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "24 fits failed out of a total of 48.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1154, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1176, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 414, in fit_transform\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 2925; got 50000. Use k='all' to return all features.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1154, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1176, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 414, in fit_transform\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 2938; got 50000. Use k='all' to return all features.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1154, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1176, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 414, in fit_transform\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 2921; got 50000. Use k='all' to return all features.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1154, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1176, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 414, in fit_transform\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 38096; got 50000. Use k='all' to return all features.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1154, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1176, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 414, in fit_transform\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 38104; got 50000. Use k='all' to return all features.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1154, in fit_transform\n",
      "    results = self._parallel_func(X, y, fit_params, _fit_transform_one)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 1176, in _parallel_func\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 414, in fit_transform\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\base.py\", line 870, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 471, in fit\n",
      "    self._check_params(X, y)\n",
      "  File \"C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\", line 668, in _check_params\n",
      "    raise ValueError(\n",
      "ValueError: k should be >=0, <= n_features = 38252; got 50000. Use k='all' to return all features.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.30708003 0.17087876 0.2433515  0.17088908 0.32121325 0.17087876\n",
      " 0.23698175 0.17088908        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;all&#x27;,\n",
       "                                        FeatureUnion(transformer_list=[(&#x27;len&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;numtans&#x27;,\n",
       "                                                                                         NumCharTransformer()),\n",
       "                                                                                        (&#x27;scale&#x27;,\n",
       "                                                                                         StandardScaler())])),\n",
       "                                                                       (&#x27;bagger&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                                         CountVectorizer(lowercase=False)),\n",
       "                                                                                        (&#x27;feature_selector&#x27;,\n",
       "                                                                                         SelectKBest(k=50000,\n",
       "                                                                                                     score_func=&lt;function chi2 at 0x000001AD1C04E8C0&gt;)),\n",
       "                                                                                        (&#x27;tfidf&#x27;,\n",
       "                                                                                         TfidfTransformer(...)),\n",
       "                                       (&#x27;mlpclassifier&#x27;,\n",
       "                                        MLPClassifier(activation=&#x27;tanh&#x27;,\n",
       "                                                      hidden_layer_sizes=(8, 6,\n",
       "                                                                          1),\n",
       "                                                      max_iter=100,\n",
       "                                                      random_state=123))]),\n",
       "             param_grid={&#x27;all__bagger__vect__analyzer&#x27;: [&#x27;word&#x27;, &#x27;char&#x27;],\n",
       "                         &#x27;all__bagger__vect__ngram_range&#x27;: [(1, 1), (1, 2)],\n",
       "                         &#x27;mlpclassifier__activation&#x27;: [&#x27;identity&#x27;, &#x27;logistic&#x27;],\n",
       "                         &#x27;mlpclassifier__early_stopping&#x27;: [True],\n",
       "                         &#x27;mlpclassifier__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;sgd&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-57\" type=\"checkbox\" ><label for=\"sk-estimator-id-57\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;all&#x27;,\n",
       "                                        FeatureUnion(transformer_list=[(&#x27;len&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;numtans&#x27;,\n",
       "                                                                                         NumCharTransformer()),\n",
       "                                                                                        (&#x27;scale&#x27;,\n",
       "                                                                                         StandardScaler())])),\n",
       "                                                                       (&#x27;bagger&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                                         CountVectorizer(lowercase=False)),\n",
       "                                                                                        (&#x27;feature_selector&#x27;,\n",
       "                                                                                         SelectKBest(k=50000,\n",
       "                                                                                                     score_func=&lt;function chi2 at 0x000001AD1C04E8C0&gt;)),\n",
       "                                                                                        (&#x27;tfidf&#x27;,\n",
       "                                                                                         TfidfTransformer(...)),\n",
       "                                       (&#x27;mlpclassifier&#x27;,\n",
       "                                        MLPClassifier(activation=&#x27;tanh&#x27;,\n",
       "                                                      hidden_layer_sizes=(8, 6,\n",
       "                                                                          1),\n",
       "                                                      max_iter=100,\n",
       "                                                      random_state=123))]),\n",
       "             param_grid={&#x27;all__bagger__vect__analyzer&#x27;: [&#x27;word&#x27;, &#x27;char&#x27;],\n",
       "                         &#x27;all__bagger__vect__ngram_range&#x27;: [(1, 1), (1, 2)],\n",
       "                         &#x27;mlpclassifier__activation&#x27;: [&#x27;identity&#x27;, &#x27;logistic&#x27;],\n",
       "                         &#x27;mlpclassifier__early_stopping&#x27;: [True],\n",
       "                         &#x27;mlpclassifier__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;sgd&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-58\" type=\"checkbox\" ><label for=\"sk-estimator-id-58\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;all&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;len&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;numtans&#x27;,\n",
       "                                                                  NumCharTransformer()),\n",
       "                                                                 (&#x27;scale&#x27;,\n",
       "                                                                  StandardScaler())])),\n",
       "                                                (&#x27;bagger&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                  CountVectorizer(lowercase=False)),\n",
       "                                                                 (&#x27;feature_selector&#x27;,\n",
       "                                                                  SelectKBest(k=50000,\n",
       "                                                                              score_func=&lt;function chi2 at 0x000001AD1C04E8C0&gt;)),\n",
       "                                                                 (&#x27;tfidf&#x27;,\n",
       "                                                                  TfidfTransformer())]))])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(8, 6, 1),\n",
       "                               max_iter=100, random_state=123))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-59\" type=\"checkbox\" ><label for=\"sk-estimator-id-59\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">all: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;len&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;numtans&#x27;,\n",
       "                                                 NumCharTransformer()),\n",
       "                                                (&#x27;scale&#x27;, StandardScaler())])),\n",
       "                               (&#x27;bagger&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                 CountVectorizer(lowercase=False)),\n",
       "                                                (&#x27;feature_selector&#x27;,\n",
       "                                                 SelectKBest(k=50000,\n",
       "                                                             score_func=&lt;function chi2 at 0x000001AD1C04E8C0&gt;)),\n",
       "                                                (&#x27;tfidf&#x27;,\n",
       "                                                 TfidfTransformer())]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>len</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-60\" type=\"checkbox\" ><label for=\"sk-estimator-id-60\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumCharTransformer</label><div class=\"sk-toggleable__content\"><pre>NumCharTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-61\" type=\"checkbox\" ><label for=\"sk-estimator-id-61\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>bagger</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-62\" type=\"checkbox\" ><label for=\"sk-estimator-id-62\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(lowercase=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-63\" type=\"checkbox\" ><label for=\"sk-estimator-id-63\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectKBest</label><div class=\"sk-toggleable__content\"><pre>SelectKBest(k=50000, score_func=&lt;function chi2 at 0x000001AD1C04E8C0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-64\" type=\"checkbox\" ><label for=\"sk-estimator-id-64\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-65\" type=\"checkbox\" ><label for=\"sk-estimator-id-65\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(8, 6, 1), max_iter=100,\n",
       "              random_state=123)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('all',\n",
       "                                        FeatureUnion(transformer_list=[('len',\n",
       "                                                                        Pipeline(steps=[('numtans',\n",
       "                                                                                         NumCharTransformer()),\n",
       "                                                                                        ('scale',\n",
       "                                                                                         StandardScaler())])),\n",
       "                                                                       ('bagger',\n",
       "                                                                        Pipeline(steps=[('vect',\n",
       "                                                                                         CountVectorizer(lowercase=False)),\n",
       "                                                                                        ('feature_selector',\n",
       "                                                                                         SelectKBest(k=50000,\n",
       "                                                                                                     score_func=<function chi2 at 0x000001AD1C04E8C0>)),\n",
       "                                                                                        ('tfidf',\n",
       "                                                                                         TfidfTransformer(...)),\n",
       "                                       ('mlpclassifier',\n",
       "                                        MLPClassifier(activation='tanh',\n",
       "                                                      hidden_layer_sizes=(8, 6,\n",
       "                                                                          1),\n",
       "                                                      max_iter=100,\n",
       "                                                      random_state=123))]),\n",
       "             param_grid={'all__bagger__vect__analyzer': ['word', 'char'],\n",
       "                         'all__bagger__vect__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'mlpclassifier__activation': ['identity', 'logistic'],\n",
       "                         'mlpclassifier__early_stopping': [True],\n",
       "                         'mlpclassifier__solver': ['lbfgs', 'sgd']})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here model parameters are being tuned\n",
    "#further some parameters of the CountVectorizer are also being tuned.\n",
    "pipeline_parameters = {\n",
    "    \"all__bagger__vect__ngram_range\": [(1,1), (1,2)],\n",
    "    \"all__bagger__vect__analyzer\": [\"word\", \"char\"],\n",
    "    \"mlpclassifier__activation\": [\"identity\", \"logistic\"],\n",
    "    \"mlpclassifier__solver\": [\"lbfgs\", \"sgd\"],\n",
    "    \"mlpclassifier__early_stopping\": [True]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, \n",
    "                    pipeline_parameters,\n",
    "                    cv = 3\n",
    "                    )\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "wEGt36ngm4rB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on Test Data: 0.6562720917445605\n"
     ]
    }
   ],
   "source": [
    "#see model prediction on test dataset\n",
    "\n",
    "#the dataset is cleaned with the same procedure as the training data\n",
    "#trivially, upsampling would not be of any use here.\n",
    "df_test = PP.clean_tweets(df_test)\n",
    "\n",
    "X_test, y_test = df_test[\"tweet\"], df_test[\"label\"]\n",
    "print(f\"Performance on Test Data: {pipeline.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "FS72pEvUomnc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix of out of sample results\n",
    "y_pred_test = pd.Series(pipeline.predict(X_test))\n",
    "conf_matr = confusion_matrix(y_test, y_pred_test, labels=pipeline.classes_, normalize = None)\n",
    "conf_matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "WGTAkl0L3ByT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiram\\AppData\\Local\\Temp\\ipykernel_2760\\2554065607.py:4: MatplotlibDeprecationWarning: Support for FigureCanvases without a required_interactive_framework attribute was deprecated in Matplotlib 3.6 and will be removed two minor releases later.\n",
      "  fig, ax = plt.subplots(figsize=(10,10))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1ad1ef5fbb0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot the confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = conf_matr,\n",
    "                              display_labels = pipeline.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "QqXMYEG27Uov"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ar       0.00      0.00      0.00       529\n",
      "     ar_LATN       0.00      0.00      0.00         3\n",
      "          az       0.00      0.00      0.00         2\n",
      "          bg       0.00      0.00      0.00         2\n",
      "          bs       0.00      0.00      0.00         1\n",
      "          ca       0.00      0.00      0.00         3\n",
      "          cs       0.00      0.00      0.00         1\n",
      "          da       0.00      0.00      0.00         1\n",
      "          de       0.00      0.00      0.00        50\n",
      "          el       0.00      0.00      0.00        11\n",
      "          en       0.95      0.95      0.95      4756\n",
      "          es       0.38      0.92      0.54      1476\n",
      "          eu       0.00      0.00      0.00         2\n",
      "          fa       0.00      0.00      0.00         5\n",
      "          fi       0.00      0.00      0.00         8\n",
      "          fr       0.00      0.00      0.00       224\n",
      "          he       0.00      0.00      0.00        14\n",
      "          hi       0.00      0.00      0.00         4\n",
      "     hi-Latn       0.00      0.00      0.00         4\n",
      "          hr       0.00      0.00      0.00         1\n",
      "          ht       0.00      0.00      0.00         1\n",
      "          id       0.00      0.00      0.00       817\n",
      "          it       0.00      0.00      0.00        76\n",
      "          ja       0.62      0.98      0.76      2476\n",
      "     ja_LATN       0.00      0.00      0.00         1\n",
      "          jv       0.00      0.00      0.00         1\n",
      "          km       0.00      0.00      0.00         1\n",
      "          ko       0.23      0.22      0.22       110\n",
      "     ko_LATN       0.00      0.00      0.00         1\n",
      "          la       0.00      0.00      0.00         1\n",
      "          lv       0.00      0.00      0.00         5\n",
      "          mk       0.00      0.00      0.00         1\n",
      "          mn       0.00      0.00      0.00         1\n",
      "          mr       0.00      0.00      0.00         1\n",
      "          ms       0.00      0.00      0.00        31\n",
      "          nl       0.00      0.00      0.00        43\n",
      "          no       0.00      0.00      0.00         1\n",
      "          pl       0.00      0.00      0.00        26\n",
      "          pt       0.00      0.00      0.00       699\n",
      "          ro       0.00      0.00      0.00         2\n",
      "          ru       0.00      0.00      0.00       243\n",
      "          sk       0.00      0.00      0.00         1\n",
      "          sr       0.00      0.00      0.00         7\n",
      "          su       0.00      0.00      0.00         0\n",
      "          sv       0.00      0.00      0.00        15\n",
      "          sw       0.00      0.00      0.00         2\n",
      "          ta       0.00      0.00      0.00         3\n",
      "     ta_LATN       0.00      0.00      0.00         1\n",
      "          th       0.00      0.00      0.00        98\n",
      "          tl       0.00      0.00      0.00        89\n",
      "          tr       0.00      0.00      0.00       174\n",
      "          uk       0.00      0.00      0.00         2\n",
      "         und       0.10      0.04      0.06       685\n",
      "          ur       0.00      0.00      0.00         5\n",
      "     ur_LATN       0.00      0.00      0.00         1\n",
      "          vi       0.00      0.00      0.00         5\n",
      "          wo       0.00      0.00      0.00         0\n",
      "          xh       0.00      0.00      0.00         1\n",
      "          yo       0.00      0.00      0.00         1\n",
      "       zh-CN       0.00      0.00      0.00         1\n",
      "       zh-TW       0.00      0.00      0.00         4\n",
      "          zu       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66     12731\n",
      "   macro avg       0.04      0.05      0.04     12731\n",
      "weighted avg       0.53      0.66      0.57     12731\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kiram\\miniconda3\\envs\\MachineLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#accuracy per label\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1-033J0k1f_QlaFje3stzlrphsd34otZV",
     "timestamp": 1665779354544
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
